# Generic Autonomous Vehicle System
Road sign pose estimation for autonomous vehicles.

# Installation
## 1 - Requirements
1. [Cmake](https://cmake.org/)
2. [Python 3.12+](https://www.python.org/) (You probably will be able to get away with an older version of Python, but I've been developing on 3.12)
3. [Git](https://git-scm.com/)
4. [CP210x USB to UART Bridge VCP Drivers](https://www.silabs.com/developers/usb-to-uart-bridge-vcp-drivers?tab=downloads)
5. [PyTorch](https://pytorch.org/) (Ultralytics will automatically install PyTorch but installing manually is recommended)
6. [Ultralytics](https://docs.ultralytics.com/) (Ultralytics will automatically be installed by setup scripts)
## 2 - Clone the Repository
```git clone --recursive https://github.com/NathanQ2/HybridStereoAutonomousVehicleSystem.git```

## 3 - Automatic Installation
### 3.1 - Windows
```
cd scripts
setupt.bat
```
### 3.2 - Mac/Linux
```
cd scripts
bash setupt.sh
```

## 4 Manual Installation
### 4.1 - Build RP_LiDAR_Interface_Cpp
#### 4.1.1 - Download and Install the Appropriate Drivers
The [CP210x USB to UART Bridge VCP Drivers](https://www.silabs.com/developers/usb-to-uart-bridge-vcp-drivers?tab=downloads) are required to communicate with the LiDAR over USB.
#### 4.1.2 - Build Using Cmake
```
cd HybridStereoAutonomousVehicleSystem
cd vendor/RP_LiDAR_Interface_Cpp
mkdir build
cd build
cmake ..
cmake --build .
cd ../../../
```
### 4.2 - Install Required Python Packages
```python3 -m pip install -r requirements.txt```
### 4.3 - Training the Model
At the moment, training configuration parameters can be edited in the [train.py](src/test/train.py) file. I trained with ```epochs=1000, device=[0], batch=-0.90``` and got alright results.

```python3 src/test/train.py```
### 4.4 - Run the [main.py](src/main/main.py) File!
#### 4.4.1 - Windows
```python3 src/main/main.py [lidar com port Ex: com3]```
#### 4.4.2 - Linux
```python3 src/main/main.py [lidar com port Ex: /dev/ttyUSB0]```
#### 4.4.3 - Mac
```python3 src/main/main.py [lidar com port Ex: /dev/ttySLAB_USBtoUART]```


# How It Works
## Hardware
I chose to use two generic usb webcams in a stereo configuration with a baseline of ~190.5 millimeters. 
This allows us to do basic depth estimation using the difference between the two images.
Camera calibration constants can be found in the [rightCameraProperties.json](cameraCalib/rightCameraProperties.json) and the [leftCameraProperties.json](cameraCalib/leftCameraProperties.json) files respectively.
The other json files were generated by [calibdb.net](https://www.calibdb.net/#).
I've also chosen to use a [SLAMTEC RP LiDAR A1](https://www.slamtec.ai/product/slamtec-rplidar-a1/) rotating LiDAR for more accurate depth perception when available.
![Setup Without LiDAR](assets/Setup_No_LiDAR.JPEG)
![Setup With LiDAR](assets/Setup_With_LiDAR.JPEG)
## Software
### Sign Detection
Sign detection is done using a pre-trained YOLOv8 model for each camera.